import streamlit as st
from pathlib import Path
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_community.chat_models import GigaChat 
import json
import os

from settings.config import GIGACHAT_TOKEN, generator_prompt, critic_prompt, qa_prompt


st.set_page_config(page_title="HypGen", layout="wide")

st.markdown("""
<style>
    /* –®—Ä–∏—Ñ—Ç Inter */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap');
    
    html, body, [class*="css"], .stMarkdown, p, li, div, span, label, input, textarea {
        font-family: 'Inter', sans-serif;
        color: #ffffff !important;
    }
    
    /* –§–æ–Ω */
    .stApp {
        background: linear-gradient(135deg, #0f0519, #1a0b33, #49165e);
        background-size: 400% 400%;
        animation: gradient 20s ease infinite;
    }
    
    @keyframes gradient {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    
    h1, h2, h3, h4, h5, h6 {
        color: #ffffff !important;
        text-align: center;
        text-shadow: 0 0 15px rgba(186, 104, 200, 0.8), 0 0 30px rgba(186, 104, 200, 0.4);
        font-weight: 700;
    }

    /* –ü–æ–ª—è –≤–≤–æ–¥–∞ */
    .stTextInput > div > div > input,
    .stTextArea > div > div > textarea {
        background-color: rgba(255, 255, 255, 0.08) !important;
        color: #ffffff !important;
        border-radius: 16px;
        border: 1px solid rgba(255, 255, 255, 0.2) !important;
        padding: 16px;
        font-weight: 500;
        backdrop-filter: blur(20px) saturate(200%) !important;
        -webkit-backdrop-filter: blur(20px) saturate(200%) !important;
        box-shadow: 
            inset 0 1px 0 rgba(255, 255, 255, 0.1),
            0 8px 32px rgba(0, 0, 0, 0.15);
        transition: all 0.3s ease;
    }
    
    .stTextInput > div > div > input:focus,
    .stTextArea > div > div > textarea:focus {
        background-color: rgba(255, 255, 255, 0.12) !important;
        border-color: rgba(186, 104, 200, 0.7) !important;
        box-shadow: 
            inset 0 1px 0 rgba(255, 255, 255, 0.15),
            0 0 0 2px rgba(186, 104, 200, 0.2),
            0 8px 32px rgba(0, 0, 0, 0.25);
        outline: none;
    }
    
    .stTextInput > div > div > input::placeholder,
    .stTextArea > div > div > textarea::placeholder {
        color: rgba(255, 255, 255, 0.6) !important;
        font-weight: 400;
    }
    
    .stTextInput > div > div,
    .stTextArea > div > div {
        background-color: transparent !important;
    }
    
    .css-1d391kg {
        background: linear-gradient(to bottom, rgba(40, 15, 80, 0.98), rgba(20, 5, 40, 0.98));
        border-right: 2px solid #ba68c8;
        box-shadow: 8px 0 30px rgba(0, 0, 0, 0.7);
        padding: 20px;
    }
    
    .streamlit-expanderHeader {
        background-color: rgba(106, 27, 154, 0.3);
        color: #ffffff;
        border-radius: 12px;
    }
    
    .stSuccess, .stInfo {
        background-color: rgba(106, 27, 154, 0.2);
        border-left: 4px solid #ba68c8;
        border-radius: 10px;
    }
    
    div.stButton > button {
        background: linear-gradient(to right, #4a148c, #6a1b9a) !important;
        color: white !important;
        border-radius: 12px;
        border: none;
        padding: 12px 24px;
        font-weight: bold;
        box-shadow: 0 4px 15px rgba(74, 20, 140, 0.5);
        transition: all 0.3s ease !important;
    }

    div.stButton > button:hover {
        background: linear-gradient(to right, #6a1b9a, #7b1fa2) !important;
        box-shadow: 0 0 20px rgba(106, 27, 154, 0.8);
        transform: translateY(-2px);
    }
</style>
""", unsafe_allow_html=True)

# –∑–∞–≥–æ–ª–æ–≤–æ–∫
st.title("HypGen")
st.markdown("<h3 style='color: #ffffff;'>—Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç —Ç–µ–∫—Å—Ç</h3>", unsafe_allow_html=True)
st.markdown("*–ó–¥–µ—Å—å –º–æ–≥–ª–∞ –±—ã—Ç—å –≤–∞—à–∞ —Ä–µ–∫–ª–∞–º–∞*")

CREDENTIALS = GIGACHAT_TOKEN

@st.cache_resource
def get_generator_llm():
    return GigaChat(
        credentials=CREDENTIALS,
        temperature=0.6,
        verify_ssl_certs=False,
        timeout=120
    )

@st.cache_resource
def get_critic_llm():
    return GigaChat(
        credentials=CREDENTIALS,
        model="GigaChat-Max",   
        temperature=0.2,        
        verify_ssl_certs=False,
        timeout=120
    )

generator_llm = get_generator_llm()
critic_llm = get_critic_llm()

GENERATOR_PROMPT = PromptTemplate.from_template(generator_prompt)
CRITIC_PROMPT = PromptTemplate.from_template(critic_prompt)
QA_PROMPT = PromptTemplate.from_template(qa_prompt)

@st.cache_resource
def load_vectorstore():
    try:
        FAISS_DIR = Path(r"C:\PROJECT\ai-agent\faiss_index")
        embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
        return FAISS.load_local(FAISS_DIR, embeddings, allow_dangerous_deserialization=True)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")
        return None

vectorstore = load_vectorstore()



# __________________________________________________________________________________
# —Å–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Ç–æ–≤. –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç 
CHAT_FILE = "chat_history.json"

def init_chat_history():
    if 'chat_history' not in st.session_state:
        if os.path.exists(CHAT_FILE) and os.path.getsize(CHAT_FILE) > 0:
            try:
                with open(CHAT_FILE, "r", encoding="utf-8") as f:
                    st.session_state.chat_history = json.load(f)
            except (json.JSONDecodeError, Exception):
                st.session_state.chat_history = {"chat_1": []}
        else:
            st.session_state.chat_history = {"chat_1": []}
    
    if 'current_chat_id' not in st.session_state:
        if st.session_state.chat_history:
            st.session_state.current_chat_id = next(iter(st.session_state.chat_history))
        else:
            st.session_state.current_chat_id = "chat_1"
            st.session_state.chat_history["chat_1"] = []
    
    # —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ–¥–Ω–µ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
    if 'last_operation' not in st.session_state:
        st.session_state.last_operation = None  
    if 'last_results' not in st.session_state:
        st.session_state.last_results = None    
    if 'last_sources' not in st.session_state:
        st.session_state.last_sources = None    
    if 'last_raw_hypotheses' not in st.session_state:
        st.session_state.last_raw_hypotheses = None  


def save_chat_history():
    try:
        with open(CHAT_FILE, "w", encoding="utf-8") as f:
            json.dump(st.session_state.chat_history, f, ensure_ascii=False, indent=4)
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —á–∞—Ç–∞: {e}")


def create_new_chat():
    st.session_state.last_operation = None
    st.session_state.last_results = None
    st.session_state.last_sources = None
    st.session_state.last_raw_hypotheses = None
    
    new_id = f"chat_{len(st.session_state.chat_history) + 1}"
    st.session_state.chat_history[new_id] = []
    st.session_state.current_chat_id = new_id
    save_chat_history()


def delete_chat(chat_id):
    if chat_id in st.session_state.chat_history:
        del st.session_state.chat_history[chat_id]
        save_chat_history()

        if st.session_state.current_chat_id == chat_id:
            if st.session_state.chat_history:
                st.session_state.current_chat_id = next(iter(st.session_state.chat_history))
            else:
                create_new_chat()
        
        st.session_state.last_operation = None
        st.session_state.last_results = None
        st.session_state.last_sources = None
        st.session_state.last_raw_hypotheses = None
        return True
    return False
# __________________________________________________________________________________






init_chat_history()

# –±–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.image("logo.svg", width='stretch')
    
    st.header("‚úâ")
    
    if st.button("‚ú¢ –ù–æ–≤—ã–π —á–∞—Ç", width='stretch'):
        create_new_chat()


    #___________________________________________________________________________________________
    chats_to_delete = [] 
    
    if not st.session_state.chat_history:
        st.info("–ù–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —á–∞—Ç–æ–≤")
    else:
        chat_ids = list(st.session_state.chat_history.keys())
        
        for chat_id in chat_ids:
            if chat_id not in st.session_state.chat_history:
                continue
                
            messages = st.session_state.chat_history[chat_id]
            
            if messages:
                first_user_msg = next((m for m in messages if m.get("role") == "user"), None)
                if first_user_msg:
                    chat_name = first_user_msg.get("content", "–ß–∞—Ç")[:30]
                    if len(first_user_msg.get("content", "")) > 30:
                        chat_name += "..."
                else:
                    chat_name = "–ü—É—Å—Ç–æ–π —á–∞—Ç"
            else:
                chat_name = "..."
            
            is_active = chat_id == st.session_state.current_chat_id
            button_style = "primary" if is_active else "secondary"
            
            col1, col2 = st.columns([4, 1])
            with col1:
                if st.button(
                    chat_name, 
                    key=f"select_{chat_id}_{chat_name}",  
                    width='stretch',
                    type=button_style
                ):
                    st.session_state.current_chat_id = chat_id
                    st.session_state.last_operation = None
                    st.session_state.last_results = None
                    st.session_state.last_sources = None
                    st.session_state.last_raw_hypotheses = None
            with col2:
                if st.button(
                    "‚úï", 
                    key=f"delete_{chat_id}_{chat_name}",  
                    help="–£–¥–∞–ª–∏—Ç—å —á–∞—Ç"
                ):
                    chats_to_delete.append(chat_id)
    
    if chats_to_delete:
        for chat_id in chats_to_delete:
            delete_chat(chat_id)
#_____________________________________________________________________________________________


    
    st.markdown("---")
    with st.expander("–û —Å–∏—Å—Ç–µ–º–µ", expanded=False):
        st.markdown("""
        **HypGen** ‚Äî –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫ –º–µ—Ç–∞–ª–ª—É—Ä–≥–∞
        
        - **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑**: GigaChat-Pro
        - **–û—Å–ø–∞—Ä–∏–≤–∞–Ω–∏–µ**: GigaChat-Max
        - **–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π**: 100+ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π (Arxiv, OpenaAlex)
        - **–ü–æ–∏—Å–∫**: FAISS + multilingual-e5-large-instruct
        """)
        st.caption(" ‚úâ 2025 | –ü—Ä–æ–µ–∫—Ç–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º")


tab1, tab2 = st.tabs(["  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑", "  –í–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç"])

with tab1:
    st.subheader("‚óã ‚óã ‚óã")
    problem = st.text_area(
        "   ",
        placeholder="...",
        height=100,
        key="problem_input"
    )
    
    col1, col2, col3 = st.columns([2, 1, 2])
    with col2:
        generate_btn = st.button("‚ñ∑ Start", type="primary", key="generate_hypotheses", width='stretch')
    
    if generate_btn and problem:
        if not vectorstore:
            st.error("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ FAISS –∏–Ω–¥–µ–∫—Å–∞.")
        else:
            with st.spinner(" –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π..."):
                try:
                    docs = vectorstore.similarity_search(problem, k=10)
                    context = "\n\n".join([
                        f"üìÑ {d.metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}:\n{d.page_content[:800]}..." 
                        for d in docs
                    ])
                    
                    with st.spinner(" –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑..."):
                        raw_response = (GENERATOR_PROMPT | generator_llm).invoke({
                            "problem": problem,
                            "context": context
                        })
                        raw_hypotheses = raw_response.content
                    
                    with st.spinner(" –û—Ü–µ–Ω–∫–∞ –≥–∏–ø–æ—Ç–µ–∑..."):
                        final_response = (CRITIC_PROMPT | critic_llm).invoke({
                            "raw_hypotheses": raw_hypotheses,
                            "context": context
                        })
                        final_hypotheses = final_response.content
                    
                    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ 
                    st.session_state.chat_history[st.session_state.current_chat_id].append({
                        "role": "user", 
                        "content": f"**–ü—Ä–æ–±–ª–µ–º–∞:** {problem}"
                    })
                    st.session_state.chat_history[st.session_state.current_chat_id].append({
                        "role": "assistant", 
                        "content": f"**–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã:**\n\n{final_hypotheses}"
                    })
                    save_chat_history()
                    
                    st.session_state.last_operation = 'generate'
                    st.session_state.last_results = final_hypotheses
                    st.session_state.last_sources = docs[:5]
                    st.session_state.last_raw_hypotheses = raw_hypotheses
                    
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑: {str(e)}")
                    st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—É –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat.")

with tab2:
    st.subheader("‚óã ‚óã ‚óã")
    question = st.text_area(
        "   ",
        placeholder=". . .",
        height=100,
        key="qa_input"
    )
    
    col1, col2, col3 = st.columns([2, 1, 2])
    with col2:
        qa_btn = st.button("‚ñ∑ Start", type="primary", key="qa_answer", width='stretch')
    
    if qa_btn and question:
        if not vectorstore:
            st.error("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ FAISS –∏–Ω–¥–µ–∫—Å–∞.")
        else:
            with st.spinner(" –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏..."):
                try:
                    docs = vectorstore.similarity_search(question, k=5)
                    context = "\n\n".join([
                        f"{d.metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}:\n{d.page_content[:1000]}..." 
                        for d in docs
                    ])
                    
                    with st.spinner(" –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞..."):
                        response = (QA_PROMPT | generator_llm).invoke({
                            "context": context, 
                            "question": question
                        })
                        answer = response.content
                    
                    # —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
                    st.session_state.chat_history[st.session_state.current_chat_id].append({
                        "role": "user", 
                        "content": f"**–í–æ–ø—Ä–æ—Å:** {question}"
                    })
                    st.session_state.chat_history[st.session_state.current_chat_id].append({
                        "role": "assistant", 
                        "content": f"**–û—Ç–≤–µ—Ç:**\n\n{answer}"
                    })
                    save_chat_history()
                    
                    st.session_state.last_operation = 'qa'
                    st.session_state.last_results = answer
                    st.session_state.last_sources = docs
                    
                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ –æ—Ç–≤–µ—Ç–∞: {str(e)}")
                    st.info("–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ GigaChat.")


if st.session_state.last_operation == 'generate':
    st.success("### –¢–æ–ø-3 –≥–∏–ø–æ—Ç–µ–∑—ã")
    st.markdown(st.session_state.last_results)
    
    with st.expander("‚åï –ò—Å—Ö–æ–¥–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã"):
        st.markdown(st.session_state.last_raw_hypotheses)
    
    with st.expander("‚òç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
        for i, d in enumerate(st.session_state.last_sources, 1):
            st.write(f"**{i}. {d.metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}**")
            if 'source' in d.metadata:
                st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {d.metadata.get('source', '')}")
            if 'year' in d.metadata:
                st.caption(f"–ì–æ–¥: {d.metadata.get('year', '')}")
            st.divider()

elif st.session_state.last_operation == 'qa':
    st.info("### üí° –û—Ç–≤–µ—Ç")
    st.markdown(st.session_state.last_results)
    
    with st.expander("‚òç –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏"):
        for i, d in enumerate(st.session_state.last_sources, 1):
            st.write(f"**{i}. {d.metadata.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')}**")
            if 'source' in d.metadata:
                st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {d.metadata.get('source', '')}")
            if 'year' in d.metadata:
                st.caption(f"–ì–æ–¥: {d.metadata.get('year', '')}")
            st.write(f"**–§—Ä–∞–≥–º–µ–Ω—Ç:** {d.page_content[:500]}...")
            st.divider()


current_messages = st.session_state.chat_history.get(st.session_state.current_chat_id, []) # –ø–æ–∫–∞–∑ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º –æ–∫–Ω–µ

if current_messages:
    st.markdown("### ‚ä≤ –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ ‚ä≥")
    for msg in current_messages:
        with st.chat_message(msg.get("role", "user")):
            st.markdown(msg.get("content", ""))

st.markdown("---")
st.caption(" ‚úâ 2025 | –ü—Ä–æ–µ–∫—Ç–Ω—ã–π –ø—Ä–∞–∫—Ç–∏–∫—É–º")