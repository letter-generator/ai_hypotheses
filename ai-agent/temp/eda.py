import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
from pathlib import Path
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# –ü—É—Ç–∏
CHUNKS_FILE = Path("C:\\PROJECT\\ai-agent\\data\\clean.jsonl")
EDA_DIR = Path("eda_plots")
EDA_DIR.mkdir(parents=True, exist_ok=True)

print("=== –ü–†–û–î–í–ò–ù–£–¢–´–ô EDA –î–õ–Ø –ú–ï–¢–ê–õ–õ–£–†–ì–ò–ß–ï–°–ö–û–ô RAG-–°–ò–°–¢–ï–ú–´ ===")
print("=" * 60)

if not CHUNKS_FILE.exists():
    print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {CHUNKS_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
    print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏ clean_and_split.py")
    exit()

# 1. –£–º–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
chunks = []
problem_lines = 0
error_samples = []
line_errors = []

with open(CHUNKS_FILE, "r", encoding="utf-8") as f:
    for i, line in enumerate(f, 1):
        if line.strip():
            try:
                data = json.loads(line)
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                if 'chunk_text' in data and isinstance(data['chunk_text'], str) and len(data['chunk_text']) > 10:
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø–æ–ª–µ–π
                    if 'chunk_text' in data:
                        data['text'] = data['chunk_text']
                    if 'chunk_tokens' in data:
                        data['tokens'] = data['chunk_tokens']
                    chunks.append(data)
                else:
                    problem_lines += 1
                    if problem_lines <= 3:
                        error_samples.append(f"–°—Ç—Ä–æ–∫–∞ {i}: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π chunk_text")
            except (json.JSONDecodeError, KeyError) as e:
                problem_lines += 1
                line_errors.append(f"–°—Ç—Ä–æ–∫–∞ {i}: {str(e)[:50]}...")

if not chunks:
    print("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ clean.jsonl ‚Äî –±–∞–∑–∞ –ø—É—Å—Ç–∞—è.")
    exit()

print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(chunks)}")
if problem_lines > 0:
    print(f"‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {problem_lines}")
    if error_samples:
        print("  –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–±–ª–µ–º:")
        for err in error_samples[:3]:
            print(f"    - {err}")

df = pd.DataFrame(chunks)

# 2. –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –° –§–û–†–ú–ê–¢–ò–†–û–í–ê–ù–ò–ï–ú
print("\n" + "=" * 60)
print("üìä –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô")
print("=" * 60)

# –°–æ–∑–¥–∞–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
if 'text' not in df.columns and 'chunk_text' in df.columns:
    df['text'] = df['chunk_text']

df['text_length'] = df['text'].str.len()

# –†–∞—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤
if 'tokens' in df.columns:
    df['token_count'] = df['tokens']
elif 'chunk_tokens' in df.columns:
    df['token_count'] = df['chunk_tokens']
else:
    # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ (4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ, ~2 –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ)
    df['token_count'] = df['text_length'] // 3

# –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
try:
    stats = {
        "–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤": len(df),
        "–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π": df['source'].nunique() if 'source' in df.columns else "N/A",
        "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–∞": f"{df['text_length'].mean():.0f} —Å–∏–º–≤–æ–ª–æ–≤",
        "–ú–µ–¥–∏–∞–Ω–Ω–∞—è –¥–ª–∏–Ω–∞": f"{df['text_length'].median():.0f} —Å–∏–º–≤–æ–ª–æ–≤",
        "–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞": f"{df['text_length'].min():.0f} —Å–∏–º–≤–æ–ª–æ–≤",
        "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞": f"{df['text_length'].max():.0f} —Å–∏–º–≤–æ–ª–æ–≤",
        "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ": f"{df['text_length'].std():.0f} —Å–∏–º–≤–æ–ª–æ–≤",
        "–°—Ä–µ–¥–Ω–µ–µ —Ç–æ–∫–µ–Ω–æ–≤": f"{df['token_count'].mean():.0f}",
        "–û–±—â–∏–π –æ–±—ä–µ–º —Ç–µ–∫—Å—Ç–∞": f"{df['text_length'].sum() / 1_000_000:.2f} –ú–ë",
        "–ü—Ä–æ—Ü–µ–Ω—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö": f"{(len(df)/(len(df)+problem_lines)*100):.1f}%"
    }
    
    for key, value in stats.items():
        print(f"  {key:30} : {value}")
except Exception as e:
    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")

# 3. –ê–ù–ê–õ–ò–ó –ü–û –ì–û–î–ê–ú (—Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ")
print("\nüìÖ –ê–ù–ê–õ–ò–ó –ü–û –ì–û–î–ê–ú –ü–£–ë–õ–ò–ö–ê–¶–ò–ô")
print("-" * 40)

if 'year' in df.columns:
    try:
        # –û—á–∏—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –≥–æ–¥–∞
        df['year_clean'] = pd.to_numeric(
            df['year'].astype(str).str.extract(r'(\d{4})')[0], 
            errors='coerce'
        )
        valid_years = df['year_clean'].dropna()
        
        if len(valid_years) > 0:
            print(f"  –°—Ç–∞—Ç–µ–π —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –≥–æ–¥–æ–º: {len(valid_years)} ({len(valid_years)/len(df)*100:.1f}%)")
            print(f"  –î–∏–∞–ø–∞–∑–æ–Ω –ª–µ—Ç: {int(valid_years.min())} - {int(valid_years.max())}")
            print(f"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π –≥–æ–¥: {int(valid_years.median())}")
            
            # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –≥–æ–¥–∞–º
            plt.figure(figsize=(14, 7))
            
            # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
            plt.subplot(1, 2, 1)
            sns.histplot(valid_years, bins=min(30, len(valid_years.unique())),
                        kde=True, color='#8A2BE2', alpha=0.7)
            plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–µ–π –ø–æ –≥–æ–¥–∞–º', fontsize=14, fontweight='bold')
            plt.xlabel('–ì–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏')
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π')
            plt.grid(True, alpha=0.3)
            
            # Box plot –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—ã–±—Ä–æ—Å–æ–≤
            plt.subplot(1, 2, 2)
            sns.boxplot(y=valid_years, color='#9370DB')
            plt.title('Box plot: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≥–æ–¥–æ–≤', fontsize=14, fontweight='bold')
            plt.ylabel('–ì–æ–¥')
            plt.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig(EDA_DIR / "year_analysis.png", dpi=150, bbox_inches='tight')
            plt.close()
            print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω: eda_plots/year_analysis.png")
        else:
            print("  ‚ö†Ô∏è  –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –≥–æ–¥–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –≥–æ–¥–æ–≤: {e}")
else:
    print("  ‚ö†Ô∏è  –ü–æ–ª–µ 'year' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

# 4. –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –î–õ–ò–ù–´ –ß–ê–ù–ö–û–í
print("\nüìè –ê–ù–ê–õ–ò–ó –î–õ–ò–ù–´ –¢–ï–ö–°–¢–û–í–´–• –ß–ê–ù–ö–û–í")
print("-" * 40)

try:
    plt.figure(figsize=(15, 10))
    
    # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞
    plt.subplot(2, 2, 1)
    sns.histplot(df['text_length'], bins=50, kde=True, color='#4B0082')
    plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã —á–∞–Ω–∫–æ–≤', fontsize=14, fontweight='bold')
    plt.xlabel('–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª—ã)')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
    plt.axvline(df['text_length'].mean(), color='red', linestyle='--', 
                label=f'–°—Ä–µ–¥–Ω–µ–µ: {df["text_length"].mean():.0f}')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # Box plot
    plt.subplot(2, 2, 2)
    sns.boxplot(y=df['text_length'], color='#9932CC')
    plt.title('Box plot –¥–ª–∏–Ω—ã —á–∞–Ω–∫–æ–≤', fontsize=14, fontweight='bold')
    plt.ylabel('–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª—ã)')
    plt.grid(True, alpha=0.3)
    
    # QQ-plot –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏
    plt.subplot(2, 2, 3)
    from scipy import stats as sp_stats
    sp_stats.probplot(df['text_length'], dist="norm", plot=plt)
    plt.title('QQ-plot: –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ—Ä–º–∞–ª—å–Ω–æ—Å—Ç–∏', fontsize=14, fontweight='bold')
    plt.grid(True, alpha=0.3)
    
    # Cumulative distribution
    plt.subplot(2, 2, 4)
    sorted_lengths = np.sort(df['text_length'])
    if len(sorted_lengths) > 1:
        yvals = np.arange(len(sorted_lengths)) / float(len(sorted_lengths) - 1)
        plt.plot(sorted_lengths, yvals, color='#8A2BE2', linewidth=2)
        plt.title('Cumulative Distribution', fontsize=14, fontweight='bold')
        plt.xlabel('–î–ª–∏–Ω–∞ (—Å–∏–º–≤–æ–ª—ã)')
        plt.ylabel('–ü—Ä–æ—Ü–µ–Ω—Ç –¥–∞–Ω–Ω—ã—Ö')
        plt.grid(True, alpha=0.3)
    else:
        plt.text(0.5, 0.5, '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö', ha='center', va='center')
        plt.title('Cumulative Distribution', fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(EDA_DIR / "chunk_length_analysis.png", dpi=150, bbox_inches='tight')
    plt.close()
    
    print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {df['text_length'].min():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞: {df['text_length'].max():.0f} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {df['text_length'].std():.0f}")
    print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω: eda_plots/chunk_length_analysis.png")
except Exception as e:
    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–ª–∏–Ω—ã —á–∞–Ω–∫–æ–≤: {e}")

# 5. –£–ú–ù–û–ï –û–ë–õ–ê–ö–û –°–õ–û–í (—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏–∏)
print("\nüî§ –ê–ù–ê–õ–ò–ó –ö–õ–Æ–ß–ï–í–´–• –¢–ï–†–ú–ò–ù–û–í")
print("-" * 40)

try:
    # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏–∏
    metallurgy_stopwords = set(STOPWORDS)
    russian_stopwords = {'–∏', '–≤', '—Å', '–Ω–∞', '–ø–æ', '—á—Ç–æ', '—ç—Ç–æ', '–¥–ª—è', '–æ—Ç', '–∏–∑', '–∫–∞–∫', '—Ç–æ', '–∂–µ', '–Ω–æ', '–∞'}
    metallurgy_stopwords.update(russian_stopwords)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
    all_text = ' '.join(df['text'].astype(str).str.lower())
    
    # –£–±–∏—Ä–∞–µ–º —Ü–∏—Ñ—Ä—ã –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞
    words = re.findall(r'\b[a-z–∞-—è]{4,}\b', all_text)
    word_freq = Counter(words)
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Åwords
    filtered_freq = {word: count for word, count in word_freq.items()
                     if word not in metallurgy_stopwords and count > 5}
    
    if filtered_freq and len(filtered_freq) > 10:
        # –û–±–ª–∞–∫–æ —Å–ª–æ–≤
        wordcloud = WordCloud(
            width=1600, height=800,
            background_color='black',
            colormap='viridis',
            max_words=200,
            stopwords=metallurgy_stopwords
        ).generate_from_frequencies(filtered_freq)
        
        plt.figure(figsize=(20, 10))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title('–ö–ª—é—á–µ–≤—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –≤ –º–µ—Ç–∞–ª–ª—É—Ä–≥–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞—Ç—å—è—Ö',
                  fontsize=24, fontweight='bold', pad=20)
        plt.tight_layout()
        plt.savefig(EDA_DIR / "technical_wordcloud.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        # –¢–æ–ø-20 —Ç–µ—Ä–º–∏–Ω–æ–≤
        top_terms = pd.Series(filtered_freq).nlargest(20)
        
        plt.figure(figsize=(12, 8))
        top_terms.sort_values().plot(kind='barh', color='#8A2BE2')
        plt.title('–¢–æ–ø-20 —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤', fontsize=16, fontweight='bold')
        plt.xlabel('–ß–∞—Å—Ç–æ—Ç–∞')
        plt.grid(True, alpha=0.3, axis='x')
        plt.tight_layout()
        plt.savefig(EDA_DIR / "top_terms.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(filtered_freq)}")
        print(f"  –¢–æ–ø-5 —Ç–µ—Ä–º–∏–Ω–æ–≤: {', '.join(list(top_terms.index[:5]))}")
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã: eda_plots/technical_wordcloud.png, eda_plots/top_terms.png")
    else:
        print("  ‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã –∏–ª–∏ –∏—Ö —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ")
except Exception as e:
    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ—Ä–º–∏–Ω–æ–≤: {e}")

# 6. –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í
print("\nüìö –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í –î–ê–ù–ù–´–•")
print("-" * 40)

if 'source' in df.columns:
    try:
        source_stats = df['source'].value_counts()
        
        plt.figure(figsize=(14, 10))
        
        # –¢–æ–ø-15 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        plt.subplot(2, 1, 1)
        top_sources = source_stats.head(15)
        max_val = top_sources.max() if not top_sources.empty else 0
        bars = plt.barh(range(len(top_sources)), top_sources.values,
                       color=plt.cm.viridis(np.linspace(0.3, 0.9, len(top_sources))))
        plt.yticks(range(len(top_sources)), top_sources.index)
        plt.gca().invert_yaxis()
        plt.title('–¢–æ–ø-15 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —á–∞–Ω–∫–æ–≤', fontsize=14, fontweight='bold')
        plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤')
        plt.grid(True, alpha=0.3, axis='x')
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –±–∞—Ä—ã
        for i, (bar, value) in enumerate(zip(bars, top_sources.values)):
            plt.text(value + max_val * 0.01, bar.get_y() + bar.get_height()/2,
                    str(value), va='center', fontsize=9)
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (Arxiv vs OpenAlex)
        plt.subplot(2, 1, 2)
        source_types = df['source'].apply(lambda x: 'arxiv' if 'arxiv' in str(x).lower() else
                                                      'openalex' if 'openalex' in str(x).lower() else 'other')
        type_counts = source_types.value_counts()
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        wedges, texts, autotexts = plt.pie(type_counts.values, labels=type_counts.index,
                                           autopct='%1.1f%%', colors=colors, startangle=90)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(EDA_DIR / "source_analysis.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"  –í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(source_stats)}")
        print(f"  –î–æ–º–∏–Ω–∏—Ä—É—é—â–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫: {source_stats.index[0] if not source_stats.empty else 'N/A'} ({source_stats.iloc[0] if not source_stats.empty else 0} —á–∞–Ω–∫–æ–≤)")
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω: eda_plots/source_analysis.png")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {e}")
else:
    print("  ‚ö†Ô∏è  –ü–æ–ª–µ 'source' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

# 7. –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó: –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ü–û –°–¢–†–ê–ù–ê–ú
print("\nüåç –ê–ù–ê–õ–ò–ó –ü–û –°–¢–†–ê–ù–ê–ú")
print("-" * 40)

if 'country' in df.columns:
    try:
        country_stats = df['country'].value_counts()
        
        if not country_stats.empty:
            plt.figure(figsize=(12, 8))
            
            # –¢–æ–ø-10 —Å—Ç—Ä–∞–Ω
            top_countries = country_stats.head(10)
            top_countries.plot(kind='bar', color='#6A5ACD')
            plt.title('–¢–æ–ø-10 —Å—Ç—Ä–∞–Ω –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å—Ç–∞—Ç–µ–π', fontsize=14, fontweight='bold')
            plt.xlabel('–°—Ç—Ä–∞–Ω–∞')
            plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3, axis='y')
            plt.tight_layout()
            plt.savefig(EDA_DIR / "country_distribution.png", dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω: {len(country_stats)}")
            print(f"  –¢–æ–ø-3 —Å—Ç—Ä–∞–Ω—ã: {', '.join(list(country_stats.index[:3]))}")
            print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω: eda_plots/country_distribution.png")
        else:
            print("  ‚ö†Ô∏è  –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ç—Ä–∞–Ω–∞–º")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å—Ç—Ä–∞–Ω: {e}")
else:
    print("  ‚ö†Ô∏è  –ü–æ–ª–µ 'country' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")

# 8. –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í
print("\nüîç –ê–ù–ê–õ–ò–ó –î–£–ë–õ–ò–ö–ê–¢–û–í –ò –ü–û–í–¢–û–†–û–í")
print("-" * 40)

try:
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Ç–µ–∫—Å—Ç—É
    text_duplicates = df['text'].duplicated().sum()
    duplicate_percentage = (text_duplicates / len(df)) * 100
    
    print(f"  –ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞: {text_duplicates} ({duplicate_percentage:.1f}%)")
    
    if text_duplicates > 0:
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        plt.figure(figsize=(10, 6))
        labels = ['–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ', '–î—É–±–ª–∏–∫–∞—Ç—ã']
        sizes = [len(df) - text_duplicates, text_duplicates]
        colors = ['#4CAF50', '#F44336']
        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏ –¥—É–±–ª–∏—Ä—É—é—â–∏—Ö—Å—è —á–∞–Ω–∫–æ–≤', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(EDA_DIR / "duplicates_analysis.png", dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω: eda_plots/duplicates_analysis.png")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–∏–µ —á–∞–Ω–∫–∏ (–ø–æ –ø–µ—Ä–≤—ã–º 100 —Å–∏–º–≤–æ–ª–∞–º)
    df['text_start'] = df['text'].str.slice(0, 100)
    start_duplicates = df['text_start'].duplicated().sum()
    print(f"  –ß–∞–Ω–∫–∏ —Å –ø–æ—Ö–æ–∂–∏–º –Ω–∞—á–∞–ª–æ–º: {start_duplicates}")
    
except Exception as e:
    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {e}")

# 9. –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó
print("\nüìà –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó")
print("-" * 40)

try:
    # –í—ã–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    # –£–±–∏—Ä–∞–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ—Å—Ç–æ—è—â–∏–µ –∏–∑ –æ–¥–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è)
    informative_numeric_cols = []
    for col in numeric_cols:
        if df[col].nunique() > 1 and not df[col].isna().all():
            informative_numeric_cols.append(col)
    
    if len(informative_numeric_cols) > 1:
        corr_matrix = df[informative_numeric_cols].corr()
        
        plt.figure(figsize=(10, 8))
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,
                    square=True, linewidths=1, cbar_kws={"shrink": 0.8},
                    fmt='.2f')
        plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(EDA_DIR / "correlation_matrix.png", dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω—ë–Ω: eda_plots/correlation_matrix.png")
        
        # –í—ã–≤–æ–¥–∏–º —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_value = corr_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # –°–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                    strong_correlations.append(
                        f"{corr_matrix.columns[i]} - {corr_matrix.columns[j]}: {corr_value:.2f}"
                    )
        
        if strong_correlations:
            print("  –°–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏:")
            for corr in strong_correlations:
                print(f"    ‚Ä¢ {corr}")
    else:
        print("  ‚ö†Ô∏è  –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
except Exception as e:
    print(f"  ‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–º –∞–Ω–∞–ª–∏–∑–µ: {e}")

# 10. –°–û–•–†–ê–ù–ï–ù–ò–ï –°–í–û–î–ù–û–ì–û –û–¢–ß–ï–¢–ê
print("\nüìÑ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢")
print("=" * 60)

try:
    report_path = EDA_DIR / "eda_summary.txt"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("=" * 60 + "\n")
        f.write("–û–¢–ß–ï–¢ EDA –î–õ–Ø –ú–ï–¢–ê–õ–õ–£–†–ì–ò–ß–ï–°–ö–û–ô RAG-–°–ò–°–¢–ï–ú–´\n")
        f.write("=" * 60 + "\n\n")
        
        f.write("1. –û–°–ù–û–í–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê\n")
        f.write("-" * 40 + "\n")
        for key, value in stats.items():
            f.write(f"{key:35} : {value}\n")
        
        f.write("\n2. –ö–ê–ß–ï–°–¢–í–û –î–ê–ù–ù–´–•\n")
        f.write("-" * 40 + "\n")
        f.write(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {problem_lines}\n")
        f.write(f"–ü—Ä–æ—Ü–µ–Ω—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {(len(df)/(len(df)+problem_lines)*100):.1f}%\n")
        f.write(f"–ü–æ–ª–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–∫—Å—Ç–∞: {text_duplicates} ({duplicate_percentage:.1f}%)\n")
        
        if 'year_clean' in df.columns:
            valid_years = df['year_clean'].dropna()
            if len(valid_years) > 0:
                f.write(f"\n3. –í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó\n")
                f.write("-" * 40 + "\n")
                f.write(f"–°—Ç–∞—Ç–µ–π —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º –≥–æ–¥–æ–º: {len(valid_years)}\n")
                f.write(f"–î–∏–∞–ø–∞–∑–æ–Ω –ª–µ—Ç: {int(valid_years.min())} - {int(valid_years.max())}\n")
                f.write(f"–ú–µ–¥–∏–∞–Ω–Ω—ã–π –≥–æ–¥: {int(valid_years.median())}\n")
                f.write(f"–°—Ä–µ–¥–Ω–∏–π –≥–æ–¥: {valid_years.mean():.1f}\n")
        
        if filtered_freq and len(filtered_freq) > 10:
            f.write(f"\n4. –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó\n")
            f.write("-" * 40 + "\n")
            f.write(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤: {len(filtered_freq)}\n")
            f.write(f"–¢–æ–ø-10 —Ç–µ—Ä–º–∏–Ω–æ–≤:\n")
            for i, (term, freq) in enumerate(list(filtered_freq.most_common(10)), 1):
                f.write(f"  {i:2}. {term:25} : {freq:5}\n")
        
        if 'source' in df.columns:
            f.write(f"\n5. –ê–ù–ê–õ–ò–ó –ò–°–¢–û–ß–ù–ò–ö–û–í\n")
            f.write("-" * 40 + "\n")
            f.write(f"–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {df['source'].nunique()}\n")
            f.write(f"–¢–æ–ø-5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:\n")
            for i, (source, count) in enumerate(df['source'].value_counts().head(5).items(), 1):
                f.write(f"  {i}. {source:50} : {count}\n")
        
        f.write(f"\n6. –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\n")
        f.write("-" * 40 + "\n")
        
        recommendations = []
        if len(df) < 100:
            recommendations.append("–ë–∞–∑–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞ (<100 —á–∞–Ω–∫–æ–≤). –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Å—Ç–∞—Ç–µ–π.")
        if text_duplicates / len(df) > 0.1:
            recommendations.append("–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (>10%). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ preprocessing.")
        if 'text_length' in df.columns and df['text_length'].std() > df['text_length'].mean() * 0.5:
            recommendations.append("–í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –¥–ª–∏–Ω—ã —á–∞–Ω–∫–æ–≤. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤.")
        if 'year_clean' in df.columns and len(df['year_clean'].dropna()) / len(df) < 0.5:
            recommendations.append("–ú–µ–Ω–µ–µ 50% —Å—Ç–∞—Ç–µ–π –∏–º–µ—é—Ç –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏. –£–ª—É—á—à–∏—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.")
        
        if recommendations:
            for i, rec in enumerate(recommendations, 1):
                f.write(f"  {i}. {rec}\n")
        else:
            f.write("  –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º—ã!\n")
        
        f.write(f"\n7. –í–´–í–û–î–´ –î–õ–Ø RAG-–°–ò–°–¢–ï–ú–´\n")
        f.write("-" * 40 + "\n")
        f.write(f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —á–∞–Ω–∫–æ–≤ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞\n")
        f.write(f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑\n")
        f.write(f"‚Ä¢ –ù–∞–ª–∏—á–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–≥–æ–¥, —Å—Ç—Ä–∞–Ω–∞) —É–ª—É—á—à–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç\n")
        
    print(f"‚úÖ –í–°–ï –ì–†–ê–§–ò–ö–ò –°–û–•–†–ê–ù–ï–ù–´ –í: {EDA_DIR}")
    print(f"‚úÖ –°–í–û–î–ù–´–ô –û–¢–ß–ï–¢: {report_path}")
    
except Exception as e:
    print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –æ—Ç—á–µ—Ç–∞: {e}")

print("\n" + "=" * 60)
print("üìå –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø –ë–ê–ó–´ –ó–ù–ê–ù–ò–ô:")
print("=" * 60)

# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
recommendations = []
if len(df) < 100:
    recommendations.append("‚ö†Ô∏è  –ë–∞–∑–∞ —Å–ª–∏—à–∫–æ–º –º–∞–ª–∞ (<100 —á–∞–Ω–∫–æ–≤). –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ —Å—Ç–∞—Ç–µ–π.")
if text_duplicates / len(df) > 0.1:
    recommendations.append("‚ö†Ô∏è  –í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ (>10%). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ preprocessing.")
if 'text_length' in df.columns and df['text_length'].std() > df['text_length'].mean() * 0.5:
    recommendations.append("‚ö†Ô∏è  –í—ã—Å–æ–∫–∞—è –¥–∏—Å–ø–µ—Ä—Å–∏—è –¥–ª–∏–Ω—ã —á–∞–Ω–∫–æ–≤. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä—É–π—Ç–µ —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤.")
if 'year_clean' in df.columns and len(df['year_clean'].dropna()) / len(df) < 0.5:
    recommendations.append("‚ö†Ô∏è  –ú–µ–Ω–µ–µ 50% —Å—Ç–∞—Ç–µ–π –∏–º–µ—é—Ç –≥–æ–¥ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏. –£–ª—É—á—à–∏—Ç–µ –ø–∞—Ä—Å–∏–Ω–≥ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö.")
if 'country' in df.columns and df['country'].nunique() < 3:
    recommendations.append("‚ö†Ô∏è  –ú–∞–ª–æ —Å—Ç—Ä–∞–Ω –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ. –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑—å—Ç–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏.")

for i, rec in enumerate(recommendations, 1):
    print(f"  {i}. {rec}")

if not recommendations:
    print("  ‚úÖ –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º—ã!")

print("\n" + "=" * 60)
print("üéØ –ö–õ–Æ–ß–ï–í–´–ï –ú–ï–¢–†–ò–ö–ò –î–õ–Ø –û–¢–ß–ï–¢–ê:")
print("=" * 60)
print(f"  ‚Ä¢ –†–∞–∑–º–µ—Ä –±–∞–∑—ã: {len(df)} —á–∞–Ω–∫–æ–≤")
print(f"  ‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ: {stats.get('–ü—Ä–æ—Ü–µ–Ω—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö', 'N/A')}")
print(f"  ‚Ä¢ –¢–µ—Ä–º–∏–Ω—ã: {len(filtered_freq) if 'filtered_freq' in locals() else 'N/A'} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö")
print(f"  ‚Ä¢ –í—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Ö–≤–∞—Ç: {int(valid_years.min()) if 'valid_years' in locals() and len(valid_years) > 0 else 'N/A'}-{int(valid_years.max()) if 'valid_years' in locals() and len(valid_years) > 0 else 'N/A'}")
print("=" * 60)